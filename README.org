#+title: Zrównoleglenie wstępnego przetwarzania danych przy użyciu OpenMPI
#+author: Karol Wójcik, Krzysztof Swałdek
#+date: 2020-10-14, Wtorek

#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [12pt]
#+LATEX_HEADER: \usepackage[a4paper, inner=37.125mm, outer=33.4125mm, top=37.125mm, bottom=37.125mm, heightrounded, marginparwidth=51pt, marginparsep=17pt, headsep=24pt]{geometry}
#+EXCLUDE_TAGS: noexport
#+LATEX_HEADER: \usepackage{graphicx}
#+LaTeX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \usepackage{mathpazo}
#+LaTeX_HEADER: \linespread{1.05}
#+LATEX_HEADER: \usepackage{setspace}
#+LATEX_HEADER: \doublespacing
#+LATEX_HEADER: \usepackage[polish]{babel}
#+LATEX_HEADER: \usepackage{polski}
#+LaTeX_HEADER: \usepackage{minted}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LaTeX_HEADER: \usemintedstyle{emacs}
#+LaTeX_HEADER: \newminted{common-lisp}{fontsize=\footnotesize}
#+LATEX_HEADER: \usepackage{ragged2e}
#+LATEX_HEADER: \usepackage{xltxtra}
#+LaTeX_HEADER: \usepackage{pdfpages}
#+OPTIONS: ^:{} ':t
#+LANGUAGE: pl
#+OPTIONS: toc:nil

#+begin_export latex
  \clearpage \tableofcontents \clearpage
#+end_export

#+name: setup-minted
#+begin_src emacs-lisp :exports none :results silent :eval yes
(setq org-latex-listings 'minted)
(setq org-latex-minted-options
      '(("frame" "single")
        ("framesep" "2mm")
        ("fontsize" "\\small")))
(setq org-latex-to-pdf-process
      '("pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"
        "pdflatex -shell-escape -interaction nonstopmode -output-directory %o %f"))
#+end_src

* Wprowadzenie
  Celem projektu jest implementacja algorytmu /Eliminacji obserwacji odstających/ do wstępnego przetwarzania danych, a także wykonanie i udokumentowanie pomiaru czasowago trzech wariantów programu.

  @@latex: \noindent @@
  1. Zrównoleglenie programu za pomocą OpenMPI
  2. Zrównoleglenie programu za pomocą OpenOMP
  3. Zrównoleglenie programu w modelu hybrydowym; połączenie OpenMPI i OpenOMP

* Opis badanego algorytmu
Algorytm /eliminacji obserwacji odstających/ korzysta z reguły trzech sigm. Reguła ta stanowi, że:

@@latex: \vspace{6mm} @@

\noindent
/*Dla zmiennej, która ma rozkład normalny bądź zbliżony do rozkładu normalnego* można wyznaczyć ile obserwacji znajduje się w pewnym zakresie mierzonym odchyleniem standardowym (od średniej)./

@@latex: \vspace{3mm} @@
\noindent
/Można zaobserwować, że:/
- /68,2% obserwacji znajduje się w zakresie pomiędzy +/- 1 odchylenie standardowe od średniej/
- /95,4% obserwacji znajduje się w zakresie pomiędzy +/- 2 odchylenia standardowe od średniej/
- /99,7% obserwacji znajduje się w zakresie pomiędzy +/- 3 odchylenia standardowe od średniej/
- /99,994% obserwacji znajduje się w zakresie pomiędzy +/- 4 odchylenia standardowe od średniej/

@@latex: \noindent @@
Zważywszy na powyższe w projekcie przyjęliśmy za wartości odstające wszystkie te, które odbiegają od średniej o +/- 3 odchylenia standardowe. Wartości odstające nie są usuwane ze zbioru, by nie powodować dziur. Za wartości odstające przyjęliśmy wartość średnią ze zbioru. \\

@@latex: \noindent @@
Operacje odpowiadające za eliminację i podmianę wartości na średnią prezentują się następująco: \\

#+ATTR_LATEX: :options
#+caption: Algorytm eliminacji obserwacji odstających
#+BEGIN_SRC c++ -i :eval no
for (auto x : xv) {
  double zscore = (x - mean_t) / std_t;

  if (zscore < 3) {
    nxv_private.push_back(x);
  } else {
    nxv_private.push_back(mean_t);
  }
}
#+END_SRC

* Metoda wykonywania pomiaru
Program dla każdego wariantu został uruchomiony trzydzieści razy na platformie uczelnianej do obliczeń rozproszonych w celu zapewnienia poprawności i statystycznej dokładności wykonywach pomiarów. Pomiary czasowe podawane są dla każdego wariantu w jednostce ~ms~ (milisekundy). Wykorzystywana bilbioteka do przeprowadzania pomiarów to ~<std::chrono>~.

@@latex: \vspace{3mm} \noindent@@
Poniżej zaprezentowano klasę wykonującą pomiary:

#+caption: Funkcja pobierająca aktualny czas w milisekundach
#+BEGIN_SRC c++ :eval no
double get_time() {
    return std::chrono::time_point_cast<std::chrono::milliseconds>(
            std::chrono::system_clock::now())
            .time_since_epoch()
            .count();
}
#+END_SRC

* Opis zbióru danych
  @@latex: \noindent @@
  Dane pochodzą z archiwum [[https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation][uci]]. Zawierają próbki kolorów G, R, B z wycinków obrazków twarzy ludzi w tym:
  - starych
  - młodych
  - w średnim wieku
  - o różnej rasie i kolorze skóry

  @@latex: \noindent @@
  Sumaryczna liczba próbek wynosi 14 799 994. Rozmiar pliku wynosi 191MB. W danych istnieją też wycinki z różnych obrazków nie przedstawiających ludzi. Oryginalnie kolumna ostatnia zawierająca informację o przynależności kolorów G, R, B do twarzy miała następującą postać: \\

  \noindent
  *SKIN*:
  - ~1~ - twarz
  - ~2~ - nie twarz

  \noindent
  Dane zostały poddane wstępnej obróbce:
  1. Zamieniono kolumny tak, by odpowiadała kolejność R, G, B
  2. Zamieniono oznaczenie *SKIN* na:
     - ~0~ - nie twarz
     - ~1~ - twarz

* Pomiary
** Zrównoleglenie programu za pomocą OpenMPI
Z pliku podanego jako wartość typu string odczytywane są trzy kolumny: ~R~, ~G~, ~B~. Następnie środowisko MPI jest inicjalizowane poprzez komendę ~MPI_INIT~ i pobrane zostają wartości odpowiadające za identyfikator procesu i liczbę dostępnych procesów. W kolejnym kroku algorytmu wyliczane zostają stałe ~COLUMN_SIZE~ oraz ~COLUMN_PARTITION_SIZE~, na których to podstawie występować będzie podział każdej z kolumny. \\

#+caption: Wyliczenie liczebności partycji
#+BEGIN_SRC c++ :eval no
int COLUMN_PARTITION_SIZE = std::ceil(
    COLUMN_SIZE / (float) processes
);

int ALL_PARTITION_COLUMN_SIZE = processes * COLUMN_PARTITION_SIZE;
#+END_SRC

@@latex: \noindent @@
Podział podzbioru w kolumnie realizowany jest poprzez użycie dwóch funkcji ~MPI_SCATTER~ oraz ~MPI_GATHER~ wraz z podanymi powyżej wartościami stałych. W kodzie programu występują trzy bloki z użyciem pary ~MPI_SCATTER~ i ~MPI_GATHER~, które synchronizują procesy w obrębie bloku. \\

#+caption[Podział]: Podział kolekcji na podprocesy
#+begin_src c++ :eval no
std::vector<double> R_partition(COLUMN_PARTITION_SIZE);
MPI_Scatter(
    R.data(), COLUMN_PARTITION_SIZE, MPI_DOUBLE,
    R_partition.data(), COLUMN_PARTITION_SIZE, MPI_DOUBLE,
    0, MPI_COMM_WORLD
);
#+end_src

@@latex: \noindent @@
Każdy proces realizuje eliminację elementów odstających dla swojej partycji, a następnie wyniki są łączone do jednego wektora.

#+caption[Procesowanie kolekcji]: Realizacja algorytmu dla każdego podprocesu
#+begin_src c++ :eval no
auto R_partition_M = std::move(
    remove_outliers(R_partition, MEAN_R, STD_R)
);
#+end_src

@@latex: \noindent @@
Ten sam kod występuje dla wszystkich kolumn, gdzie jedyną różnicą są zmienne określające kolejne kolumny. \\

#+caption[Akumulacja]: Zebranie danych podprocesu do procesu głównego
#+begin_src c++ :eval no
MPI_Gather(
    R_partition_M.data(), COLUMN_PARTITION_SIZE, MPI_DOUBLE,
    R_new.data(), COLUMN_PARTITION_SIZE, MPI_DOUBLE,
    0, MPI_COMM_WORLD
);
#+END_SRC

@@latex: \vspace{3mm} \noindent@@
Poniżej przedstawiono statystykę wydajności w postaci tabel oraz wykresów dla środowiska *MPI*. \\

#+caption: Statystyka dla 30 uruchomień, wartości w ms
#+ATTR_LaTeX: :align |c|c|c|c|
|---------+---------+------+------|
| Mediana | Średnia | NKCW | NDCW |
|---------+---------+------+------|
|   1101  |  1167   | 918  | 1899 |
|---------+---------+------+------|
- NKCW - najkrótszy czas wykonania
- NDCW - najdłuższy czas wykonania

** Zrównoleglenie programu za pomocą OpenOMP

Zbiór danych z pliku podanego jako wartość parametru wejściowego typu string, odczytywane są pokolei kolumny (~R~, ~G~, ~B~), z natychmiastowym wywołaniem funkcji eliminacji obserwacji odstających. Następnie w zrównoleglony sposób przy użyciu ~#pragma omp for~, które zrównolegla pętlę for, jest obliczana średnia.

#+caption: Obliczanie średniej oraz podział na wątki
#+begin_src c++ :eval no
    double sum = 0;
    #pragma omp parallel for schedule(runtime)
    for (auto x : xv) {
      sum += x;
    }

    return sum / xv.size()
#+END_SRC

@@latex: \vspace{3mm} \noindent @@
Dalszym krokiem jest przejście do sedna algorytmu eliminacji obserwacji odstających, gdzie sama, operacja jest wykonywana równolegle przy wykorzystaniu ~#pragma omp parallel~, a dodatkowo jest dzielona na kolejne części za pomocą ~#pragma omp for~. Do podziału iteracji użyto dyrektywy ~schedule~, która ustala sposób rozdzielania iteracji pomiędzy wątki. Ustawiona została wartość ~runtime~, oznaczająca ustalanie liczebność każdego zbioru w czasie dzialania programu. Po wykonaniu się algorytmu, następuje sekcja krytyczna zwracająca wyniki. \\

#+caption: Uruchomienie zadania eliminacji obserwacji odstających w osobnym wątku oraz podział na kolejne wątki
#+begin_src c++ :eval no
#pragma omp parallel
    {
      std::vector<double> nxv_private;

      #pragma omp for schedule(runtime) nowait
      for (auto x : xv) {
        double zscore = (x - mean_t) / std_t;

        if (zscore < 3) {
          nxv_private.push_back(x);
        } else {
          nxv_private.push_back(mean_t);
        }
      }

      #pragma omp critical
      nxv.insert(nxv.end(), nxv_private.begin(), nxv_private.end());
    }
#+END_SRC

@@latex: \vspace{3mm} \noindent@@
Poniżej przedstawiono statystykę wydajności w postaci tabel oraz wykresów dla środowiska *OpenOMP*. \\

#+caption: Statystyka dla 30 uruchomień, wartości w ms
#+ATTR_LaTeX: :align |c|c|c|c|
|---------+---------+------+------|
| Mediana | Średnia | NKCW | NDCW |
|---------+---------+------+------|
|  20702  | 20941   | 20289|23193 |
|---------+---------+------+------|
- NKCW - najkrótszy czas wykonania
- NDCW - najdłuższy czas wykonania


** Zrównoleglenie programu w modelu hybrydowym
Zrównoleglenie to będzie połączeniem obydwóch metod, *MPI* oraz *OpenOMP*. Zbiór danych z pliku podanego jako wartość parametru wejsciowego typu string, odczytywane są trzy kolumny: ~R~, ~G~, ~B~. Następnie obliczane sa średnie przy pomocy zrównoleglenia za pomocą *OpenOMP*.

#+caption: Obliczanie średniej wartości oraz podział zadania na wątki
#+begin_src c++ :eval no
double sum = 0;

#pragma omp parallel for schedule(runtime)
for (auto x : xv) {
    sum += x;
}

return sum / xv.size()
#+END_SRC

@@latex: \vspace{3mm} \noindent @@
Dalszym krokiem jest inicjalizacja środowiska MPI w taki sam sposób, jak przy użyciu samego ~OpenMPI~. Później następuje podział zbioru danych na mniejsze podzbiory, który jest realizowany za pomoca funkcji ~MPI_SCATTER~ oraz ~MPI_GATHER~.

#+caption: Podział na podprocesy
#+begin_src c++ :eval no
#pragma omp parallel
std::vector<double> R_partition(COLUMN_PARTITION_SIZE);

MPI_Scatter(R.data(), COLUMN_PARTITION_SIZE,
            MPI_DOUBLE, R_partition.data(),
            COLUMN_PARTITION_SIZE, MPI_DOUBLE,
            0, MPI_COMM_WORLD);
 #+END_SRC


@@latex: \noindent @@
Wewnątrz jednego podzbioru użyty został jeszcze *OpenOMP*. \\

#+caption: Uruchamianie zadania eliminacji obserwacji odstających oraz podział na wątki
#+begin_src c++ :eval no
#pragma omp parallel
    {
        std::vector<double> nxv_private;

        #pragma omp for schedule(runtime) nowait
        for (auto x : xv) {
            double zscore = (x - mean_t) / std_t;

            if (zscore < 3) {
                nxv_private.push_back(x);
            } else {
                nxv_private.push_back(mean_t);
            }
        }

        #pragma omp critical
        nxv.insert(
            nxv.end(),
            nxv_private.begin(),
            nxv_private.end()
        );
    }
#+END_SRC


@@latex: \noindent @@
Na końcu zostaja zebrane dane z podprocesów do procesu głównego.

#+caption: Zbieranie danych z podprocesów do procesu głównego
#+begin_src c++ :eval no
MPI_Gather(
    R_partition_M.data(), COLUMN_PARTITION_SIZE,
    MPI_DOUBLE, R_new.data(),
    COLUMN_PARTITION_SIZE, MPI_DOUBLE,
    0, MPI_COMM_WORLD
);
#+END_SRC

@@latex: \vspace{3mm} \noindent@@
Poniżej przedstawiono statystykę wydajności w postaci tabel oraz wykresów dla podejścia hybrydowego. \\

#+caption: Statystyka dla 30 uruchomień, wartości w ms
#+ATTR_LaTeX: :align |c|c|c|c|
|---------+---------+------+------|
| Mediana | Średnia | NKCW | NDCW |
|---------+---------+------+------|
|  668    |  673    | 624  | 766  |
|---------+---------+------+------|
- NKCW - najkrótszy czas wykonania
- NDCW - najdłuższy czas wykonania

** Podsumowanie pomiarów
#+caption: Statystyka dla 30 uruchomień, wartości w ms
#+ATTR_LaTeX: :align |c|c|c|c|c|
|------------+---------+---------+------+------|
| Środowisko | Mediana | Średnia | NKCW | NDCW |
|------------+---------+---------+------+------|
|   OpenMPI  |   1101  |  1167   | 918  | 1899 |
|------------+---------+---------+------+------|
|   OpenOMP  |  20702  | 20941   | 20289|23193 |
|------------+---------+---------+------+------|
|   Hybrid   |  668    |  673    | 624  | 766  |
|------------+---------+---------+------+------|
- NKCW - najkrótszy czas wykonania
- NDCW - najdłuższy czas wykonania

#+attr_latex: :width 0.8\textwidth
#+caption: [MPI] Pomiar dla wszystkich wariantów bez skalowania
[[./resources/all1.png]]

#+attr_latex: :width 0.8\textwidth
#+caption: [MPI] Pomiar dla wszystkich wariantów z skalą logatytmiczną
[[./resources/all.png]]

#+attr_latex: :width 0.8\textwidth
#+caption: [MPI] Pomiar dla wszystkich wariantów bez skalowania
[[./resources/cw.png]]

#+attr_latex: :width 0.8\textwidth
#+caption: [MPI] Pomiar dla wszystkich wariantów z skalą logatytmiczną
[[./resources/cwlog.png]]

#+attr_latex: :width 0.8\textwidth
#+caption: [MPI] Przyspiesznie dla wszystkich wariantów bez skalowania
[[./resources/p.png]]

#+attr_latex: :width 0.8\textwidth
#+caption: [MPI] Przyspiesznie dla wszystkich wariantów z skalą logatytmiczną
[[./resources/plog.png]]

#+attr_latex: :width 0.8\textwidth
#+caption: [MPI] Pomiar dla najlepszych wyników bez skalowania
[[./resources/cwBest.png]]

#+attr_latex: :width 0.8\textwidth
#+caption: [MPI] Przyspieszenie dla najlepszych wynikow z skalą logatytmiczną
[[./resources/pBest.png]]

@@latex: \clearpage @@
* Podsumowanie
Najmniej skuteczną metoda przyspieszania obliczeń okazało sie środowisko *OpenOMP*. Zapewnia ono stosunkowo jedno z najprostszych metod zrównoleglania, gdzie na etapie projektowania programu nie trzeba mysleć o prarelizacji. Kolejne miejsce zajął *OpenMPI*, które nestety cechuje się znacznym nakładem pracy przy zrównoleglaniu, lecz efekty są dużo lepsze w porównaniu do poprzedniego środowiska. Hybrydowe jest w tym wypadku najoptymalniejszą wersją programu. Podejście hybrydowe pozwala wykorzystać mocne strony każdego ze środowisk, co przekłada się na korzystny wynik czasowy.
